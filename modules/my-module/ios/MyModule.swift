import ExpoModulesCore
import AVFoundation

let BUF_PER_SEC = 15

public class MyModule: Module {
  // MARK: - Audio session & engines
  private let audioSession = AVAudioSession.sharedInstance()
  private var sessionConfigured = false          // âœ… ì„¸ì…˜ì€ ìµœì´ˆ 1íšŒë§Œ ì„¤ì •
  private let sampleRate: Double = 24_000        // ì›í•˜ëŠ” ì¶œë ¥(ì¬ìƒ) ìƒ˜í”Œë ˆì´íŠ¸
  private let channels: UInt32 = 1

  private let audioEngine = AVAudioEngine()       // ë…¹ìŒìš© ì—”ì§„
  private var converter: AVAudioConverter?

  private let playerEngine = AVAudioEngine()      // ì¬ìƒìš© ì—”ì§„
  private var sourceNode: AVAudioSourceNode?
  private var isPlaying = false

  // MARK: - Buffer & queue
  private var pcmDataQueue = DispatchQueue(label: "pcm.audio.queue", qos: .userInteractive)
  private var pcmBufferQueue = [Data]()
  private let maxQueueLength = 100

  private var currentPCMData: UnsafeMutablePointer<Int16>?
  private var currentDataLength: Int = 0
  private var currentReadPosition: Int = 0

  private var previousSample: Float = 0.0
  private let fadeLength: Int = 64
  private var isFirstBlock = true

  private var silenceFramesGenerated: Int = 0
  private let maxSilenceFrames: Int = 1024

  private var didInitialRouteChangeHandled = false

  private var lastFrameEventTime: CFTimeInterval = 0

  // MARK: - Module definition
  public func definition() -> ModuleDefinition {
    Name("MyModule")
    Constants([
      "BUF_PER_SEC": BUF_PER_SEC
    ])

    Events("onChange", "onAudioBuffer", "onMicAudio", "onRecordingSaved", "onAudioRouteChange", "onRecordingReady", "onPlaybackFrame")

    // public API
    Function("startRecording") { self.startRecording() }
    Function("stopRecording") { self.stopRecording() }
    Function("startRealtimePlayback") { self.startRealtimePlayback() }
    Function("stopRealtimePlayback") { self.stopRealtimePlayback() }
    Function("pauseRealtimePlayback") { self.pauseRealtimePlayback() }
    Function("resumeRealtimePlayback") { self.resumeRealtimePlayback() }
    Function("streamPCMData") { (data: Data) in self.streamPCMData(data) }
    Function("playPCMBuffer") { (data: Data) in self.playPCMBuffer(data) }

    // ìµœì´ˆ ìƒì„± ì‹œ ë‹¨ í•œ ë²ˆ ì˜¤ë””ì˜¤ ì„¸ì…˜ êµ¬ì„± & ë…¸í‹° ë“±ë¡
    OnCreate {
      do {
        try self.setupAudioSessionIfNeeded()
      } catch {
        print("âŒ AudioSession ì´ˆê¸°í™” ì‹¤íŒ¨: \(error)")
      }

      NotificationCenter.default.addObserver(
        self,
        selector: #selector(self.handleAudioRouteChange(_:)),
        name: AVAudioSession.routeChangeNotification,
        object: nil
      )
    }
  }

  deinit {
    pauseRealtimePlayback()
    stopRecording()
    NotificationCenter.default.removeObserver(self, name: AVAudioSession.routeChangeNotification, object: nil)
  }

  // MARK: - AudioSession (oneâ€‘time)
  private func setupAudioSessionIfNeeded() throws {
    guard !sessionConfigured else { return }

    try audioSession.setCategory(.playAndRecord,
                                 mode: .voiceChat,
                                 options: [.defaultToSpeaker, .allowBluetooth, .mixWithOthers])
    try audioSession.setPreferredSampleRate(sampleRate)
    try audioSession.setPreferredIOBufferDuration(0.005)
    try audioSession.setActive(true)               // ğŸ‘‰ ë‹¨ í•œ ë²ˆë§Œ í™œì„±í™”

    sessionConfigured = true
    print("ğŸ”§ AudioSession configured once (actual SR: \(audioSession.sampleRate))")
  }

  // MARK: - Routeâ€‘change handler
  @objc private func handleAudioRouteChange(_ notification: Notification) {
    guard let userInfo = notification.userInfo,
          let reasonValue = userInfo[AVAudioSessionRouteChangeReasonKey] as? UInt,
          let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else { return }

    // ì²« routeâ€‘change (ì„¸ì…˜ í™œì„±í™” ì§í›„) ë¬´ì‹œ
    if !didInitialRouteChangeHandled {
      didInitialRouteChangeHandled = true
      print("ğŸ‘‹ ìµœì´ˆ ì˜¤ë””ì˜¤ ê²½ë¡œ ë³€ê²½ ë¬´ì‹œ (reason: \(reason))")
      return
    }

    let routeDescription = audioSession.currentRoute.outputs.map { $0.portType.rawValue }.joined(separator: ", ")
    sendEvent("onAudioRouteChange", ["reason": "\(reason)", "newRoute": routeDescription])
  }

  // MARK: - ë…¹ìŒ -------------------------------------------------------------
  func startRecording() {
    audioSession.requestRecordPermission { granted in
      guard granted else {
        print("âŒ ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨")
        return
      }

      DispatchQueue.main.async {
        do {
          try self.setupAudioSessionIfNeeded()   // ì´ë¯¸ êµ¬ì„±ëë‹¤ë©´ noâ€‘op

          let inputNode = self.audioEngine.inputNode
          try? inputNode.setVoiceProcessingEnabled(true)

          let hwFormat = inputNode.inputFormat(forBus: 0)            // ì‹¤ì œ HW í¬ë§·(48 kHz ë“±)
          let outputFormat = AVAudioFormat(commonFormat: .pcmFormatInt16,
                                           sampleRate: 16_000,
                                           channels: 1,
                                           interleaved: true)!
          self.converter = AVAudioConverter(from: hwFormat, to: outputFormat)

          if inputNode.outputFormat(forBus: 0).channelCount > 0 {
            inputNode.removeTap(onBus: 0)
          }

          inputNode.installTap(onBus: 0, bufferSize: 256, format: hwFormat) { buffer, _ in
            guard let converter = self.converter else { return }

            let frameCapacity = AVAudioFrameCount(Float(buffer.frameLength) * Float(outputFormat.sampleRate / hwFormat.sampleRate))
            guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: frameCapacity) else { return }

            var error: NSError?
            let status = converter.convert(to: convertedBuffer, error: &error) { _, outStatus in
              outStatus.pointee = .haveData
              return buffer
            }
            guard status == .haveData, let channelData = convertedBuffer.int16ChannelData else { return }

            let pcmData = Data(bytes: channelData[0], count: Int(convertedBuffer.frameLength) * MemoryLayout<Int16>.size)
            self.sendEvent("onMicAudio", ["pcm": pcmData])
          }

          try self.audioEngine.start()
          print("ğŸ™ï¸ ë…¹ìŒ ì‹œì‘ë¨")
          self.sendEvent("onRecordingReady", [:])
        } catch {
          print("âŒ ë…¹ìŒ ì„¤ì • ì‹¤íŒ¨: \(error)")
        }
      }
    }
  }

  func stopRecording() {
    audioEngine.inputNode.removeTap(onBus: 0)
    audioEngine.stop()
    converter = nil
    print("â¹ï¸ ë…¹ìŒ ì¤‘ì§€ë¨")
  }

  // MARK: - ì‹¤ì‹œê°„ ì¬ìƒ ------------------------------------------------------
  func startRealtimePlayback() {
    DispatchQueue.main.async {
      do {
        try self.setupAudioSessionIfNeeded()     // ì´ë¯¸ êµ¬ì„±ëë‹¤ë©´ noâ€‘op
        self.prepareOutputEngine()
        try self.playerEngine.start()
        self.isPlaying = true
        self.isFirstBlock = true
        self.silenceFramesGenerated = 0
        print("ğŸš€ ì‹¤ì‹œê°„ ì¬ìƒ ì‹œì‘ (sampleRate: \(self.sampleRate))")
      } catch {
        print("âŒ ì‹¤ì‹œê°„ ì¬ìƒ ì‹œì‘ ì‹¤íŒ¨: \(error)")
      }
    }
  }

  func stopRealtimePlayback() {
    print("â¹ï¸ ì‹¤ì‹œê°„ ì¬ìƒ ì¤‘ì§€")
    isPlaying = false

    if playerEngine.isRunning { playerEngine.stop() }
    if let node = sourceNode {
      playerEngine.disconnectNodeInput(node)
      playerEngine.detach(node)
    }
    sourceNode = nil

    pcmDataQueue.sync {
      currentPCMData?.deallocate(); currentPCMData = nil
      currentDataLength = 0; currentReadPosition = 0
      pcmBufferQueue.removeAll()
      isFirstBlock = true; silenceFramesGenerated = 0
    }

    print("âœ… ì‹¤ì‹œê°„ ì¬ìƒ ì •ë¦¬ ì™„ë£Œ (AudioSession ìœ ì§€)")
  }

  func pauseRealtimePlayback() {
    print("â¸ï¸ ì‹¤ì‹œê°„ ì¬ìƒ ì¼ì‹œ ì¤‘ì§€")
    isPlaying = false
    if playerEngine.isRunning { playerEngine.stop() }
    if let node = sourceNode {
      playerEngine.disconnectNodeInput(node)
      playerEngine.detach(node)
    }
    sourceNode = nil
    print("âœ… ì¬ìƒ ì—”ì§„ë§Œ ì •ì§€ (ë²„í¼ ë³´ì¡´ë¨)")
  }

  func resumeRealtimePlayback() {
    print("â–¶ï¸ ì‹¤ì‹œê°„ ì¬ìƒ ì¬ê°œ ì‹œë„")
    DispatchQueue.main.async {
      do {
        try self.setupAudioSessionIfNeeded()   // noâ€‘op if already done
        self.prepareOutputEngine()
        try self.playerEngine.start()
        self.isPlaying = true
        self.isFirstBlock = true
        print("âœ… ì‹¤ì‹œê°„ ì¬ìƒ ì¬ê°œ ì„±ê³µ")
      } catch {
        print("âŒ ì¬ìƒ ì¬ê°œ ì‹¤íŒ¨: \(error)")
      }
    }
  }

  private func prepareOutputEngine() {
    // ê¸°ì¡´ ë…¸ë“œ ì •ë¦¬
    if let existing = sourceNode {
      playerEngine.disconnectNodeInput(existing)
      playerEngine.detach(existing)
    }

    // Float32 í¬ë§·ìœ¼ë¡œ SourceNode ìƒì„± (24 kHz)
    let format = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                               sampleRate: sampleRate,
                               channels: channels,
                               interleaved: false)!

    sourceNode = AVAudioSourceNode(format: format) { [weak self] _, _, frameCount, audioBufferList in
      return self?.renderAudioFrames(frameCount: frameCount, audioBufferList: audioBufferList) ?? noErr
    }

    guard let node = sourceNode else {
      print("âŒ AVAudioSourceNode ìƒì„± ì‹¤íŒ¨")
      return
    }

    playerEngine.attach(node)
    playerEngine.connect(node, to: playerEngine.mainMixerNode, format: format)
    print("ğŸ›ï¸ AVAudioSourceNode ì„¤ì • ì™„ë£Œ (format: \(format))")
  }

  // MARK: - PCM ìŠ¤íŠ¸ë¦¬ë° & ë Œë”ë§ -------------------------------------------
  func streamPCMData(_ data: Data) {
    guard isPlaying else {
      print("âš ï¸ ì¬ìƒ ì¤‘ì´ ì•„ë‹ˆë¯€ë¡œ ë°ì´í„° ë¬´ì‹œë¨")
      return
    }
    guard data.count > 0 && data.count % MemoryLayout<Int16>.size == 0 else {
      print("âŒ ì˜ëª»ëœ PCM ë°ì´í„° í˜•ì‹")
      return
    }

    pcmDataQueue.async {
      if self.pcmBufferQueue.count < self.maxQueueLength {
        self.pcmBufferQueue.append(data)
        self.silenceFramesGenerated = 0
      } else {
        print("âš ï¸ ë²„í¼ ê°€ë“ ì°¸ â†’ ìƒˆ ë°ì´í„° ë¬´ì‹œ")
      }
    }
  }

  func playPCMBuffer(_ pcmData: Data) {
    print("ğŸ§ playPCMBuffer í˜¸ì¶œë¨ (\(pcmData.count) bytes)")
    if !isPlaying {
      startRealtimePlayback()
      DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
        self.streamPCMData(pcmData)
      }
    } else {
      streamPCMData(pcmData)
    }
  }

  // MARK: - ë‚´ë¶€ ë²„í¼ ë¡œë”© / ìƒ˜í”Œ ë Œë” --------------------------------------
  private func loadNextBuffer() {
    currentPCMData?.deallocate(); currentPCMData = nil

    if pcmBufferQueue.isEmpty {
      // ë¬´ìŒ ì±„ìš°ê¸° (24 kHz ê¸°ì¤€ 1/BUF_PER_SEC ì´ˆ ë¶„ëŸ‰)
      let silenceSampleCount = Int(sampleRate / Double(BUF_PER_SEC))
      let silenceData = Data(count: silenceSampleCount * MemoryLayout<Int16>.size)
      pcmBufferQueue.append(silenceData)
    }

    guard let nextData = pcmBufferQueue.first else {
      currentDataLength = 0; currentReadPosition = 0; return
    }
    pcmBufferQueue.removeFirst()

    let sampleCount = nextData.count / MemoryLayout<Int16>.size
    guard sampleCount > 0 else {
      currentDataLength = 0; currentReadPosition = 0; return
    }

    currentPCMData = UnsafeMutablePointer<Int16>.allocate(capacity: sampleCount)
    currentDataLength = sampleCount
    currentReadPosition = 0

    nextData.withUnsafeBytes { bytes in
      let int16Ptr = bytes.bindMemory(to: Int16.self)
      currentPCMData?.initialize(from: int16Ptr.baseAddress!, count: sampleCount)
    }
  }

  private func renderAudioFrames(frameCount: AVAudioFrameCount, audioBufferList: UnsafeMutablePointer<AudioBufferList>) -> OSStatus {
    let ablPointer = UnsafeMutableAudioBufferListPointer(audioBufferList)
    var accumulatedEnergy: Float = 0.0
    var peakSample: Float = 0.0
    pcmDataQueue.sync {
      for buffer in ablPointer {
        let frameData = buffer.mData!.bindMemory(to: Float.self, capacity: Int(frameCount))

        for frame in 0..<Int(frameCount) {
          var sample: Float = 0.0

          if currentPCMData == nil || currentReadPosition >= currentDataLength {
            loadNextBuffer()
          }

          if let pcm = currentPCMData, currentReadPosition < currentDataLength {
            let int16Sample = pcm[currentReadPosition]
            sample = Float(int16Sample) / 32768.0
            // Softâ€‘clip headroom
            sample = max(min(sample, 0.95), -0.95)
            currentReadPosition += 1
            silenceFramesGenerated = 0
          } else {
            // ë¬´ìŒ ë˜ëŠ” ì”í–¥
            sample = (silenceFramesGenerated < maxSilenceFrames) ? 0.0 : previousSample * 0.95
            silenceFramesGenerated += 1
          }

          // ì²« ë¸”ë¡ í˜ì´ë“œâ€‘ì¸
          if isFirstBlock && frame < fadeLength { sample *= Float(frame) / Float(fadeLength) }

          frameData[frame] = sample
          previousSample = sample

          accumulatedEnergy += sample * sample
          peakSample = max(peakSample, abs(sample))
        }
      }

      if isFirstBlock { isFirstBlock = false }
      let now = CACurrentMediaTime()
      let rms = sqrt(accumulatedEnergy / Float(frameCount))
      if now - lastFrameEventTime > 1.0 / 60.0 && rms > 0  {
        self.sendEvent("onPlaybackFrame", ["level": rms])
        lastFrameEventTime = now
      }
    }
    return noErr
  }
}